# FlexiMart Data Architecture Project

**Course:** Data for Artificial Intelligence  
**Module:** AI Data Architecture Design and Implementation  

**Student Name:** Princy Mishra  
**Student ID:**  [bitsom_ba_25071374]
**Email:** [princymishra116@gmail.com]  

---

## ğŸ“Œ Project Overview

This project demonstrates the complete design and implementation of a modern data architecture solution for **FlexiMart**, a fictional e-commerce company.  
The work covers the entire data lifecycle â€” from raw CSV ingestion and cleaning, to relational database design, NoSQL analysis, and finally a dimensional data warehouse built for analytics.

The objective of this assignment is to showcase practical data engineering skills including **ETL development**, **schema design**, **SQL analytics**, **NoSQL modeling**, and **OLAP reporting**.

---

## ğŸ§± Architecture Summary

The project is divided into three major layers:

1. **Operational Data Layer (OLTP)**
   - Raw CSV files
   - Cleaned and loaded into a relational database (MySQL)

2. **NoSQL Layer**
   - Product catalog modeled using MongoDB
   - Flexible schema with embedded documents

3. **Analytics Layer (OLAP)**
   - Star schema data warehouse
   - Analytical queries for business insights

---

## ğŸ“‚ Repository Structure

bitsom_ba_25071374-fleximart-data-architecture/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ customers_raw.csv
â”‚ â”œâ”€â”€ products_raw.csv
â”‚ â””â”€â”€ sales_raw.csv
â”‚
â”œâ”€â”€ part1-database-etl/
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ etl_pipeline.py
â”‚ â”œâ”€â”€ schema_documentation.md
â”‚ â”œâ”€â”€ business_queries.sql
â”‚ â”œâ”€â”€ data_quality_report.txt
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ part2-nosql/
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ nosql_analysis.md
â”‚ â”œâ”€â”€ mongodb_operations.js
â”‚ â””â”€â”€ products_catalog.json
â”‚
â”œâ”€â”€ part3-datawarehouse/
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ star_schema_design.md
â”‚ â”œâ”€â”€ warehouse_schema.sql
â”‚ â”œâ”€â”€ warehouse_data.sql
â”‚ â””â”€â”€ analytics_queries.sql
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## âš™ï¸ Technologies Used

- **Programming Language:** Python 3.x  
- **Libraries:** pandas, mysql-connector-python  
- **Relational Database:** MySQL 8.0  
- **NoSQL Database:** MongoDB 6.0  
- **Query Languages:** SQL, MongoDB Aggregation Framework  

---

## ğŸš€ Setup & Execution Instructions

### 1ï¸âƒ£ Database Setup (MySQL)

```bash
# Create required databases
mysql -u root -p -e "CREATE DATABASE fleximart;"
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"

2ï¸âƒ£ Run ETL Pipeline (Part 1)

pip install -r part1-database-etl/requirements.txt
python part1-database-etl/etl_pipeline.py

This will:
-- Read raw CSV files
-- Clean and standardize data
-- Load data into MySQL tables
--Generate data_quality_report.txt

3ï¸âƒ£ Execute Business Queries
mysql -u root -p fleximart < part1-database-etl/business_queries.sql

4ï¸âƒ£ MongoDB Operations (Part 2)
mongosh < part2-nosql/mongodb_operations.js

5ï¸âƒ£ Data Warehouse Setup (Part 3)
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql

ğŸ“Š Key Features Implemented
âœ” ETL Pipeline

-- Duplicate removal
-- Missing value handling
-- Date and category standardization
-- Automated data quality reporting

âœ” Relational Database Design

-- Fully normalized schema (3NF)
-- Referential integrity using foreign keys

âœ” Business Analytics (SQL)

-- Customer purchase analysis
-- Product category performance
-- Monthly sales trends with cumulative revenue

âœ” NoSQL Modeling

-- Flexible product schema
-- Embedded reviews
-- Aggregation-based analysis

âœ” Data Warehouse & OLAP

-- Star schema implementation
-- Time-based drill-down analysis
-- Product and customer segmentation analytics

ğŸ“˜ Key Learnings

-- Designing robust ETL pipelines for imperfect real-world data
-- Choosing between relational and NoSQL databases based on use cases
-- Applying dimensional modeling concepts for analytics
-- Writing efficient SQL for both OLTP and OLAP systems

âš ï¸ Challenges Faced & Solutions

- Handling inconsistent source data
  -- Solved by implementing validation, standardization, and default handling in the ETL layer.

- Designing a scalable analytics model
  -- Solved by using a star schema with surrogate keys and proper granularity.

ğŸ§¾ Submission Notes

-- All SQL scripts run without errors
-- Foreign key constraints are maintained
-- Code is commented and modular
-- Commit history follows assignment guidelines

ğŸ“ Author
-- Princy Mishra















